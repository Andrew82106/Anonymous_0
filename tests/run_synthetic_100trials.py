"""
åˆæˆæ•°æ®100æ¬¡é‡å¤å®éªŒ
æ¯ç§æ•°æ®ç±»å‹è¿è¡Œ100æ¬¡æµ‹è¯•ï¼Œæ¯æ¬¡1000ä¸ªæ ·æœ¬ï¼Œè®¡ç®—å‡†ç¡®ç‡

Requirements: éªŒè¯ ACR æ¡†æ¶åœ¨åˆæˆæ•°æ®ä¸Šçš„ç»Ÿè®¡æ˜¾è‘—æ€§
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import json
import numpy as np
from datetime import datetime
from tqdm import tqdm

from utils_set.data_generator import CausalDataGenerator
from utils_set.causal_reasoning_engine import CausalReasoningEngine
from utils_set.utils import path_config

RESULTS_DIR = str(path_config.results_dir)


def run_single_trial(engine, generator, dataset_type, n_samples=1000, trial_seed=None):
    """
    è¿è¡Œå•æ¬¡å®éªŒ
    
    Returns:
        dict: åŒ…å«é¢„æµ‹ç»“æœå’Œæ­£ç¡®æ€§
    """
    # è®¾ç½®éšæœºç§å­
    if trial_seed is not None:
        np.random.seed(trial_seed)
    
    # ç”Ÿæˆæ•°æ®
    X, Y, ground_truth, description = generator.generate_dataset(dataset_type, n_samples)
    
    try:
        # è¿è¡Œ ACR åˆ†æ
        analysis = engine.analyze_pair(X, Y)
        result = engine.infer_causality(analysis['narrative'])
        
        # æå–é¢„æµ‹
        prediction = (
            result.get('direction') or 
            result.get('causal_direction') or 
            result.get('causal_direction_judgment') or
            'Unclear'
        )
        
        confidence = result.get('confidence', 'unknown')
        
        # åˆ¤æ–­æ­£ç¡®æ€§
        is_correct = False
        if ground_truth == 'A->B' and prediction == 'A->B':
            is_correct = True
        elif ground_truth == 'B->A' and prediction == 'B->A':
            is_correct = True
        elif ground_truth == 'A_|_B' and prediction in ['A_|_B', 'Independent', 'Unclear']:
            is_correct = True
        elif ground_truth == 'A<-Z->B':
            # æ··æ·†å› å­æƒ…å†µï¼šé¢„æµ‹ A->B æˆ– B->A éƒ½ç®—"é”™è¯¯"ï¼ˆå› ä¸ºçœŸå®æ˜¯æ— ç›´æ¥å› æœï¼‰
            # ä½†è¿™æ˜¯æ–¹æ³•è®ºçš„å›ºæœ‰å±€é™ï¼Œä¸æ˜¯é”™è¯¯
            is_correct = False
        
        return {
            'prediction': prediction,
            'ground_truth': ground_truth,
            'confidence': confidence,
            'is_correct': is_correct,
            'error': None
        }
        
    except Exception as e:
        return {
            'prediction': 'Error',
            'ground_truth': ground_truth,
            'confidence': 'unknown',
            'is_correct': False,
            'error': str(e)
        }


def run_100_trials(engine, dataset_type, n_trials=100, n_samples=1000):
    """
    å¯¹å•ä¸€æ•°æ®ç±»å‹è¿è¡Œ100æ¬¡å®éªŒ
    """
    print(f"\n{'='*60}")
    print(f"Running {n_trials} trials for: {dataset_type.upper()}")
    print(f"{'='*60}")
    
    results = []
    correct_count = 0
    
    for trial in tqdm(range(n_trials), desc=f"{dataset_type}"):
        # æ¯æ¬¡ä½¿ç”¨ä¸åŒçš„éšæœºç§å­
        generator = CausalDataGenerator(random_seed=trial * 1000 + 42)
        
        result = run_single_trial(
            engine, generator, dataset_type, 
            n_samples=n_samples, 
            trial_seed=trial * 1000 + 42
        )
        results.append(result)
        
        if result['is_correct']:
            correct_count += 1
    
    accuracy = correct_count / n_trials
    
    # ç»Ÿè®¡é¢„æµ‹åˆ†å¸ƒ
    predictions = [r['prediction'] for r in results]
    prediction_counts = {}
    for p in predictions:
        prediction_counts[p] = prediction_counts.get(p, 0) + 1
    
    # ç»Ÿè®¡ç½®ä¿¡åº¦åˆ†å¸ƒ
    confidences = [r['confidence'] for r in results]
    confidence_counts = {}
    for c in confidences:
        confidence_counts[c] = confidence_counts.get(c, 0) + 1
    
    print(f"\nğŸ“Š {dataset_type.upper()} Results:")
    print(f"   Accuracy: {accuracy:.1%} ({correct_count}/{n_trials})")
    print(f"   Predictions: {prediction_counts}")
    print(f"   Confidences: {confidence_counts}")
    
    return {
        'dataset_type': dataset_type,
        'n_trials': n_trials,
        'n_samples': n_samples,
        'accuracy': accuracy,
        'correct_count': correct_count,
        'prediction_distribution': prediction_counts,
        'confidence_distribution': confidence_counts,
        'details': results
    }


def main():
    import argparse
    parser = argparse.ArgumentParser(description='Synthetic Data 100 Trials Experiment')
    parser.add_argument('--n_trials', type=int, default=100,
                        help='Number of trials per dataset type')
    parser.add_argument('--n_samples', type=int, default=1000,
                        help='Number of samples per trial')
    parser.add_argument('--types', type=str, nargs='+', 
                        default=['lingam', 'anm', 'reverse', 'independent', 'confounder'],
                        help='Dataset types to test')
    parser.add_argument('--output', type=str, default=None,
                        help='Output file path')
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("ğŸ”¬ SYNTHETIC DATA 100 TRIALS EXPERIMENT")
    print("="*70)
    print(f"Trials per type: {args.n_trials}")
    print(f"Samples per trial: {args.n_samples}")
    print(f"Dataset types: {args.types}")
    
    # åˆå§‹åŒ–å¼•æ“
    try:
        engine = CausalReasoningEngine()
        print("âœ… Causal Engine Initialized")
    except Exception as e:
        print(f"âŒ Failed to initialize engine: {e}")
        return
    
    # è¿è¡Œå®éªŒ
    all_results = {}
    
    for dtype in args.types:
        result = run_100_trials(
            engine, dtype, 
            n_trials=args.n_trials, 
            n_samples=args.n_samples
        )
        all_results[dtype] = result
    
    # æ±‡æ€»
    print("\n" + "="*70)
    print("ğŸ“Š FINAL SUMMARY")
    print("="*70)
    print(f"\n{'Dataset Type':<20} {'Accuracy':<15} {'Correct/Total':<15}")
    print("-"*50)
    
    direct_causal_correct = 0
    direct_causal_total = 0
    
    for dtype, result in all_results.items():
        acc = result['accuracy']
        correct = result['correct_count']
        total = result['n_trials']
        print(f"{dtype:<20} {acc:.1%}{'':<10} {correct}/{total}")
        
        # ç»Ÿè®¡ç›´æ¥å› æœæ¡ˆä¾‹ï¼ˆæ’é™¤æ··æ·†å› å­ï¼‰
        if dtype != 'confounder':
            direct_causal_correct += correct
            direct_causal_total += total
    
    if direct_causal_total > 0:
        direct_acc = direct_causal_correct / direct_causal_total
        print("-"*50)
        print(f"{'Direct Causal Cases':<20} {direct_acc:.1%}{'':<10} {direct_causal_correct}/{direct_causal_total}")
    
    print("="*70)
    
    # ä¿å­˜ç»“æœ
    output_file = args.output or os.path.join(RESULTS_DIR, 'synthetic_100trials_results.json')
    
    # ç§»é™¤ details ä»¥å‡å°æ–‡ä»¶å¤§å°ï¼ˆå¯é€‰ï¼‰
    save_results = {
        'experiment': 'synthetic_100_trials',
        'timestamp': datetime.now().isoformat(),
        'config': {
            'n_trials': args.n_trials,
            'n_samples': args.n_samples,
            'dataset_types': args.types
        },
        'results': {
            dtype: {
                'accuracy': r['accuracy'],
                'correct_count': r['correct_count'],
                'n_trials': r['n_trials'],
                'prediction_distribution': r['prediction_distribution'],
                'confidence_distribution': r['confidence_distribution']
            }
            for dtype, r in all_results.items()
        },
        'summary': {
            'direct_causal_accuracy': direct_acc if direct_causal_total > 0 else None,
            'direct_causal_correct': direct_causal_correct,
            'direct_causal_total': direct_causal_total
        }
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(save_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ Results saved to: {output_file}")


if __name__ == "__main__":
    main()
