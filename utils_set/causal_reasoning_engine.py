"""
å› æœæ¨ç†å¼•æ“ (Causal Reasoning Engine)
ç»“åˆ StatTranslator å’Œ LLMManagerï¼Œå®ç°ç«¯åˆ°ç«¯çš„å› æœå‘ç°æµæ°´çº¿
"""

import json
import sys
from typing import Dict, List, Optional, Any
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° sys.path
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from utils_set.stat_translator import StatTranslator
from utils_set.data_generator import CausalDataGenerator
from llms.manager import llm_manager
from utils_set.causal_inference_schema import CausalInferenceResponse
from utils_set.prompts import get_prompt
from utils_set.utils import config_loader

class CausalReasoningEngine:
    """
    å› æœæ¨ç†å¼•æ“ï¼šå°†ç»Ÿè®¡ç‰¹å¾ç¿»è¯‘ä¸ºå™äº‹ï¼Œå¹¶ä½¿ç”¨ LLM è¿›è¡Œæ¨ç†
    """
    
    def __init__(self, model_name: str = None, prompt_template: str = "sherlock"):
        """
        Parameters:
        -----------
        model_name : str
            è¦ä½¿ç”¨çš„ LLM æ¨¡å‹åç§°ï¼ˆéœ€åœ¨ config.yaml ä¸­é…ç½®ï¼‰
        prompt_template : str
            Prompt æ¨¡æ¿ç±»å‹ï¼š'sherlock', 'simple', 'residual_only'
        """
        self.translator = StatTranslator()
        
        # å¦‚æœæœªæŒ‡å®šæ¨¡å‹ï¼Œä»é…ç½®æ–‡ä»¶è¯»å–é»˜è®¤æ¨¡å‹
        if model_name is None:
            self.model_name = config_loader.get('used_model', 'deepseek-ai/DeepSeek-V3.1')
            print(f"ğŸ“ Using default model from config: {self.model_name}")
        else:
            self.model_name = model_name
        
        self.prompt_template = prompt_template
        
        # éªŒè¯æ¨¡å‹æ˜¯å¦å¯ç”¨
        available_models = llm_manager.list_models()
        if self.model_name not in available_models:
            raise ValueError(f"Model '{self.model_name}' not found. Available: {available_models}")
        
        print(f"âœ… Initialized CausalReasoningEngine with model: {self.model_name}")
    
    def analyze_pair(self, X, Y, narrative_mode: str = "full") -> Dict[str, Any]:
        """
        åˆ†æä¸€å¯¹å˜é‡å¹¶ç”Ÿæˆç»Ÿè®¡å™äº‹
        
        Parameters:
        -----------
        X, Y : array-like
            è¦åˆ†æçš„å˜é‡å¯¹
        narrative_mode : str
            å™äº‹æ¨¡å¼ï¼š'full' (å®Œæ•´), 'low_order' (ä»…ä½é˜¶ç»Ÿè®¡é‡), 'raw' (åŸå§‹æ•°å€¼)
        
        Returns:
        --------
        dict : åŒ…å«ç»Ÿè®¡æ•°æ®å’Œå™äº‹æ–‡æœ¬
        """
        stats = self.translator.analyze(X, Y)
        narrative = self.translator.generate_narrative(stats, mode=narrative_mode)
        
        return {
            'stats': stats,
            'narrative': narrative
        }
    
    def infer_causality(self, narrative: str, use_structured_output: bool = False) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM æ¨æ–­å› æœå…³ç³»
        
        Parameters:
        -----------
        narrative : str
            ç»Ÿè®¡å™äº‹æ–‡æœ¬
        use_structured_output : bool
            æ˜¯å¦ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºï¼ˆé»˜è®¤ä¸º Falseï¼Œå› ä¸ºè®¸å¤šå…¼å®¹ API æ”¯æŒä¸å®Œå–„ï¼‰
        
        Returns:
        --------
        dict : LLM çš„æ¨ç†ç»“æœ
        """
        prompt = get_prompt(self.prompt_template, narrative)
        
        # å¼ºåˆ¶åœ¨ Prompt ä¸­è¦æ±‚ JSON
        prompt += "\n\nIMPORTANT: Please ensure your response is a valid JSON object."
        
        try:
            if use_structured_output:
                # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º (ä»…å½“ç¡®ä¿¡ API æ”¯æŒè‰¯å¥½æ—¶ä½¿ç”¨)
                try:
                    model = llm_manager.get_model(self.model_name)
                    response = model.generate_structured(
                        prompt=prompt,
                        response_format=CausalInferenceResponse
                    )
                    if response:
                        return response.model_dump()
                except Exception as e:
                    print(f"âš ï¸  Structured output failed ({str(e)}), falling back to text...")
            
            # é»˜è®¤ï¼šä½¿ç”¨æ™®é€šæ–‡æœ¬è¾“å‡ºå¹¶æ‰‹åŠ¨è§£æ
            text_response = llm_manager.call_model(self.model_name, prompt, mode='text')
            return self._parse_text_response(text_response)
                
        except Exception as e:
            print(f"âŒ Error in LLM inference: {e}")
            return {
                'direction': 'Unclear',
                'confidence': 'low',
                'primary_evidence': f'Error: {str(e)}',
                'reasoning_chain': 'LLM call failed',
                'error': str(e)
            }
    
    def _parse_text_response(self, text: str) -> Dict[str, Any]:
        """
        è§£æ LLM çš„æ–‡æœ¬å“åº”ï¼ˆå¦‚æœä¸æ˜¯ç»“æ„åŒ–è¾“å‡ºï¼‰
        å°è¯•æå– JSONï¼Œæˆ–è¿”å›åŸå§‹æ–‡æœ¬
        """
        try:
            # å°è¯•æŸ¥æ‰¾ JSON å—
            start = text.find('{')
            end = text.rfind('}') + 1
            if start != -1 and end > start:
                json_str = text[start:end]
                return json.loads(json_str)
            else:
                # å¦‚æœæ‰¾ä¸åˆ° JSONï¼Œè¿”å›åŸå§‹æ–‡æœ¬
                return {
                    'direction': 'Unclear',
                    'confidence': 'low',
                    'primary_evidence': 'Failed to parse',
                    'reasoning_chain': text,
                    'raw_response': text
                }
        except json.JSONDecodeError:
            return {
                'direction': 'Unclear',
                'confidence': 'low',
                'primary_evidence': 'JSON parse error',
                'reasoning_chain': text,
                'raw_response': text
            }
    
    def run_experiment(self, datasets: List[Dict], save_results: bool = True, 
                      output_file: str = 'llm_inference_results.json') -> List[Dict]:
        """
        å¯¹ä¸€æ‰¹æ•°æ®é›†è¿è¡Œå®Œæ•´çš„æ¨ç†æµç¨‹
        
        Parameters:
        -----------
        datasets : List[Dict]
            æ•°æ®é›†åˆ—è¡¨ï¼ˆæ¥è‡ª CausalDataGeneratorï¼‰
        save_results : bool
            æ˜¯å¦ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        output_file : str
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
        Returns:
        --------
        List[Dict] : æ¯ä¸ªæ•°æ®é›†çš„å®Œæ•´æ¨ç†ç»“æœ
        """
        results = []
        
        print("\n" + "="*80)
        print(f"ğŸš€ Starting Causal Reasoning Experiment with {len(datasets)} datasets")
        print(f"   Model: {self.model_name}")
        print(f"   Prompt Template: {self.prompt_template}")
        print("="*80)
        
        for i, ds in enumerate(datasets, 1):
            print(f"\n[{i}/{len(datasets)}] Processing: {ds['name']} | Ground Truth: {ds['ground_truth']}")
            
            # æ­¥éª¤ 1: ç”Ÿæˆç»Ÿè®¡å™äº‹
            analysis = self.analyze_pair(ds['X'], ds['Y'])
            
            # æ­¥éª¤ 2: LLM æ¨ç†
            inference = self.infer_causality(analysis['narrative'], use_structured_output=False)
            
            # æ­¥éª¤ 3: è¯„ä¼°
            ground_truth = ds['ground_truth']
            
            # å…¼å®¹ä¸åŒçš„å­—æ®µå (LLM æœ‰æ—¶ä¼šè¿”å› 'causal_direction' è€Œä¸æ˜¯ 'direction')
            predicted = (
                inference.get('direction') or 
                inference.get('causal_direction') or 
                inference.get('causal_direction_judgment') or
                inference.get('judgment') or
                inference.get('å› æœæ–¹å‘åˆ¤æ–­') or
                'Unclear'
            )
            
            is_correct = (predicted == ground_truth)
            
            result = {
                'dataset_name': ds['name'],
                'ground_truth': ground_truth,
                'description': ds['description'],
                'llm_prediction': predicted,
                'llm_confidence': inference.get('confidence', 'unknown'),
                'is_correct': is_correct,
                'primary_evidence': inference.get('primary_evidence', ''),
                'reasoning_chain': inference.get('reasoning_chain', ''),
                'statistical_signals': inference.get('statistical_signals', {}),
                'narrative': analysis['narrative'],
                'full_llm_response': inference
            }
            
            results.append(result)
            
            # æ‰“å°ç®€è¦ç»“æœ
            status = "âœ… CORRECT" if is_correct else "âŒ WRONG"
            if ground_truth not in ['A->B', 'B->A']:
                status = "âš ï¸  SPECIAL CASE"
            
            print(f"   Predicted: {predicted} ({inference.get('confidence', 'N/A')}) | {status}")
        
        # è®¡ç®—ç»Ÿè®¡
        causal_cases = [r for r in results if r['ground_truth'] in ['A->B', 'B->A']]
        if causal_cases:
            correct_count = sum(1 for r in causal_cases if r['is_correct'])
            accuracy = correct_count / len(causal_cases) * 100
            
            print("\n" + "="*80)
            print("ğŸ“Š EXPERIMENT SUMMARY")
            print("="*80)
            print(f"Total Datasets: {len(datasets)}")
            print(f"Causal Cases (A->B or B->A): {len(causal_cases)}")
            print(f"Correct Predictions: {correct_count}/{len(causal_cases)}")
            print(f"Accuracy: {accuracy:.1f}%")
            print("="*80)
        
        # ä¿å­˜ç»“æœ
        if save_results:
            output_path = Path(project_root) / output_file
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ Results saved to: {output_path}")
        
        return results
